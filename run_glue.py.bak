import os, sys
from pathlib import Path
from functools import partial
from inspect import isclass
import random
from IPython.core.debugger import set_trace as bk
import pandas as pd
import numpy as np
import torch

import datasets

from argparse import ArgumentParser
from transformers import AutoTokenizerFast, AutoConfig
from _utils.wsc_trick import *  # importing spacy model takes time
from fastai.text.all import *


def setup_config(args):

    c = MyConfig(
        {
            # enable a single gpu
            "device": "cuda:0",
            # perform a single experiment
            "start": 0,
            "end": 0,
            # retrieve the model and set the seed
            "pretrained_checkpoint": args.model_name_or_path,
            "seeds": [args.seed],
            # setup optimizer params
            "lr": args.learning_rate,
            "weight_decay": args.weight_decay,
            "max_length": args.max_seq_length,
            # TODO check these parameters
            "adam_bias_correction": False,
            "xavier_reinited_outlayer": True,
            "schedule": "original_linear",
            "original_lr_layer_decays": True,
            "double_unordered": True,
            # TO MODIFY ==================================================================================
            # whether to do finetune or test
            "do_finetune": args.do_train,  # True -> do finetune ; False -> do test
            # finetuning checkpoint for testing. These will become "ckp_dir/{task}_{group_name}_{th_run}.pth"
            "th_run": {
                "qqp": 7,
                "qnli": 5,
                "mrpc": 7,
                "mnli": 2,
                "ax": 2,
                "sst2": 3,
                "rte": 7,
                "wnli": 0,
                "cola": 1,
                "stsb": 8,
            },
            "size": "small",
            # ==================================================================================
            "wsc_trick": args.wsc_trick,
            "num_workers": 3,
            # TO MODIFY ==========================================================
            "my_model": False,  # True only for my personal research
            "logger": "wandb",
            "group_name": None,  # the name of represents these runs
            # None: use name of checkpoint.
            # False: don't do online logging and don't save checkpoints
            # ==========================================================
        }
    )

    return c


def get_datasets(c):

    glue_dsets = {}
    glue_dls = {}
    for task in [
        "cola",
        "sst2",
        "mrpc",
        "stsb",
        "mnli",
        "qqp",
        "qnli",
        "rte",
        "wnli",
        "ax",
    ]:

        # Load / download datasets.
        dsets = datasets.load_dataset("glue", task, cache_dir="./datasets")

        # There is two samples broken in QQP training set
        if task == "qqp":
            dsets["train"] = dsets["train"].filter(
                lambda e: e["question2"] != "",
                cache_file_name=os.path.join(
                    dsets["train"].cache_directory(), "fixed_train.arrow"
                ),
            )

        # Load / Make tokenized datasets
        tok_func = partial(
            tokenize_sents_max_len, cols=TEXT_COLS[task], max_len=c.max_length
        )
        glue_dsets[task] = dsets.my_map(
            tok_func, cache_file_names=f"tokenized_{c.max_length}_{{split}}"
        )

        if c.double_unordered and task in ["mrpc", "stsb"]:
            swap_tok_func = partial(
                tokenize_sents_max_len,
                cols=TEXT_COLS[task],
                max_len=c.max_length,
                swap=True,
            )
            swapped_train = dsets["train"].my_map(
                swap_tok_func, cache_file_name=f"swapped_tokenized_{c.max_length}_train"
            )
            glue_dsets[task]["train"] = datasets.concatenate_datasets(
                [glue_dsets[task]["train"], swapped_train]
            )

        # Load / Make dataloaders
        hf_dsets = HF_Datasets(
            glue_dsets[task],
            hf_toker=hf_tokenizer,
            n_inp=3,
            cols={
                "inp_ids": TensorText,
                "attn_mask": noop,
                "token_type_ids": noop,
                "label": TensorCategory,
            },
        )
        if c.double_unordered and task in ["mrpc", "stsb"]:
            dl_kwargs = {
                "train": {"cache_name": f"double_dl_{c.max_length}_train.json"}
            }
        else:
            dl_kwargs = None
        glue_dls[task] = hf_dsets.dataloaders(
            bs=32,
            shuffle_train=True,
            num_workers=c.num_workers,
            cache_name=f"dl_{c.max_length}_{{split}}.json",
            dl_kwargs=dl_kwargs,
        )

    # %%
    if c.wsc_trick:
        wsc = datasets.load_dataset("super_glue", "wsc", cache_dir="./datasets")
        glue_dsets["wnli"] = wsc.my_map(
            partial(wsc_trick_process, hf_toker=hf_tokenizer),
            cache_file_names="tricked_{split}.arrow",
        )
        cols = {
            "prefix": TensorText,
            "suffix": TensorText,
            "cands": TensorText,
            "cand_lens": noop,
            "label": TensorCategory,
        }
        glue_dls["wnli"] = HF_Datasets(
            glue_dsets["wnli"], hf_toker=hf_tokenizer, n_inp=4, cols=cols
        ).dataloaders(bs=32, cache_name="dl_tricked_{split}.json")

    # %% [markdown]
    # ## 1.2 View Data
    # - View raw data on [nlp-viewer]! (https://huggingface.co/nlp/viewer/)
    #
    # - View task description on Tensorflow dataset doc for GLUE (https://www.tensorflow.org/datasets/catalog/glue)

    # %%
    if False:

        print(
            "CoLA (The Corpus of Linguistic Acceptability) - 0: unacceptable, 1: acceptable"
        )
        print(
            "Dataset size (train/valid/test): {}/{}/{}".format(
                *[len(dl.dataset) for dl in glue_dls["cola"].loaders]
            )
        )
        glue_dls["cola"].show_batch(max_n=1)
        print()
        print("SST-2 (The Stanford Sentiment Treebank) - 1: positvie, 0: negative")
        print(
            "Dataset size (train/valid/test): {}/{}/{}".format(
                *[len(dl.dataset) for dl in glue_dls["sst2"].loaders]
            )
        )
        glue_dls["sst2"].show_batch(max_n=1)
        print()
        print("MRPC (Microsoft Research Paraphrase Corpus) -  1: match, 0: no")
        print(
            "Dataset size (train/valid/test): {}/{}/{}".format(
                *[len(dl.dataset) for dl in glue_dls["mrpc"].loaders]
            )
        )
        glue_dls["mrpc"].show_batch(max_n=1)
        print()
        print("STS-B (Semantic Textual Similarity Benchmark) - 0.0 ~ 5.0")
        print(
            "Dataset size (train/valid/test): {}/{}/{}".format(
                *[len(dl.dataset) for dl in glue_dls["stsb"].loaders]
            )
        )
        glue_dls["stsb"].show_batch(max_n=1)
        print()
        print("QQP (Quora Question Pairs) - 0: no, 1: duplicated")
        print(
            "Dataset size (train/valid/test): {}/{}/{}".format(
                *[len(dl.dataset) for dl in glue_dls["qqp"].loaders]
            )
        )
        glue_dls["qqp"].show_batch(max_n=1)
        print()
        print(
            "MNLI (The Multi-Genre NLI Corpus) - 0: entailment, 1: neutral, 2: contradiction"
        )
        print(
            "Dataset size (train/validation_matched/validation_mismatched/test_matched/test_mismatched): {}/{}/{}/{}/{}".format(
                *[len(dl.dataset) for dl in glue_dls["mnli"].loaders]
            )
        )
        glue_dls["mnli"].show_batch(max_n=1)
        print()
        print(
            "(QNLI (The Stanford Question Answering Dataset) - 0: entailment, 1: not_entailment)"
        )
        print(
            "Dataset size (train/valid/test): {}/{}/{}".format(
                *[len(dl.dataset) for dl in glue_dls["qnli"].loaders]
            )
        )
        glue_dls["qnli"].show_batch(max_n=1)
        print()
        print("RTE (Recognizing_Textual_Entailment) - 0: entailment, 1: not_entailment")
        print(
            "Dataset size (train/valid/test): {}/{}/{}".format(
                *[len(dl.dataset) for dl in glue_dls["rte"].loaders]
            )
        )
        glue_dls["rte"].show_batch(max_n=1)
        print()
        print("WSC (The Winograd Schema Challenge) - 0: wrong, 1: correct")
        # There are three style, WNLI (casted in NLI type), WSC, WSC with candidates (trick used by Roberta)
        "Note for WSC trick: cands is the concatenation of candidates, cand_lens is the lengths of candidates in order."
        print(
            "Dataset size (train/valid/test): {}/{}/{}".format(
                *[len(dl.dataset) for dl in glue_dls["wnli"].loaders]
            )
        )
        glue_dls["wnli"].show_batch(max_n=1)
        print()
        print(
            "AX (GLUE Diagnostic Dataset) - 0: entailment, 1: neutral, 2: contradiction"
        )
        print(
            "Dataset size (test): {}".format(
                *[len(dl.dataset) for dl in glue_dls["ax"].loaders]
            )
        )
        glue_dls["ax"].show_batch(max_n=1)


def main(args):

    # setup the configuration for the fintuning tasks
    c = setup_config(args)

    # Check
    if not c.do_finetune:
        assert c.th_run["mnli"] == c.th_run["ax"]
    assert c.schedule in [
        "original_linear",
        "separate_linear",
        "one_cycle",
        "adjusted_one_cycle",
    ]

    # huggingface/transformers
    hf_tokenizer = AutoTokenizerFast.from_pretrained(c.pretrained_checkpoint)
    electra_config = AutoConfig.from_pretrained(c.pretrained_checkpoint)

    # %%
    METRICS = {
        **{task: [MatthewsCorrCoef()] for task in ["cola"]},
        **{
            task: [accuracy]
            for task in ["sst2", "mnli", "qnli", "rte", "wnli", "snli", "ax"]
        },
        **{task: [F1Score(), accuracy] for task in ["mrpc", "qqp"]},
        **{task: [PearsonCorrCoef(), SpearmanCorrCoef()] for task in ["stsb"]},
    }
    NUM_CLASS = {
        **{task: 1 for task in ["stsb"]},
        **{task: 2 for task in ["cola", "sst2", "mrpc", "qqp", "qnli", "rte", "wnli"]},
        **{task: 3 for task in ["mnli", "ax"]},
    }
    TEXT_COLS = {
        **{task: ["question", "sentence"] for task in ["qnli"]},
        **{
            task: ["sentence1", "sentence2"] for task in ["mrpc", "stsb", "wnli", "rte"]
        },
        **{task: ["question1", "question2"] for task in ["qqp"]},
        **{task: ["premise", "hypothesis"] for task in ["mnli", "ax"]},
        **{task: ["sentence"] for task in ["cola", "sst2"]},
    }
    LOSS_FUNC = {
        **{
            task: CrossEntropyLossFlat()
            for task in [
                "cola",
                "sst2",
                "mrpc",
                "qqp",
                "mnli",
                "qnli",
                "rte",
                "wnli",
                "ax",
            ]
        },
        **{task: MyMSELossFlat(low=0.0, high=5.0) for task in ["stsb"]},
    }
    if c.wsc_trick:
        LOSS_FUNC["wnli"] = ELECTRAWSCTrickLoss()
        METRICS["wnli"] = [wsc_trick_accuracy]


if __name__ == "__main__":

    parser = ArgumentParser()

    # Global level parameters (model and data)
    parser.add_argument("--model_name_or_path", type=str, required=True)
    parser.add_argument("--task_name", type=str, required=True)
    parser.add_argument("--do_train", action="store_true")
    parser.add_argument("--do_eval", action="store_true")
    parser.add_argument("--max_seq_length", type=int, default=128)

    parser.add_argument("--per_device_train_batch_size", type=int, default=128)
    parser.add_argument("--per_device_eval_batch_size", type=int, default=128)

    parser.add_argument("--learning_rate", type=float, default=3e-04)
    parser.add_argument("--weight_decay", type=float, default=0.8)
    parser.add_argument("--fp16", action="store_true")

    parser.add_argument("--num_train_epochs", type=int, default=3)
    parser.add_argument("--output_dir", type=str, required=True)
    parser.add_argument("--seed", type=int, default=123)

    parser.add_argument("--wsc_trick", action="store_true")

    """[summary]
        --model_name_or_path $MODELPATH \
        --task_name $TASK_NAME \
        --do_train \
        --do_eval \
        --max_seq_length $MSL \
        --per_device_train_batch_size $PDTS \
        --per_device_eval_batch_size $PDES \
        --learning_rate $LR \
        --fp16 $FP16 \
        --num_train_epochs $NEPOCHS \
        --output_dir $OUTPUTDIR/GLUE_RESULTS/$TASK_NAME/$MODELNAME/V$i \
        --seed "${i}${i}${i}"
    """

    # get NameSpace of paramters
    args = parser.parse_args()

    main(args)
